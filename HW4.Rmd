---
title: "Data622 - Group2 - Homework4"
author: "Zachary Palmore, Kevin Potter, Amit Kapoor, Adam Gersowitz, Paul Perez"
date: "10/21/2021"
output:
  html_document:
    fig_width: 15
    highlight: pygments
    number_sections: no
    theme: flatly
    toc: yes
    toc_float: yes
  pdf_document:
    latex_engine: xelatex
    toc: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width = 10)
```


# Overview

In this project, we analyze a real-life mental health dataset to provide context around suicide prediction given a variety of unidentifiable demographic data. Our goals are to understand the variables relationships, identify those variables that influence our target, and develop models that can predict a patient's risk of suicide. 

# Approach
We will first perform exploratory data analysis (EDA) on the dataset to inform our analysis and build better models. Methods include Clustering, Principal Component Analysis, Gradient Boosting, and Support Vector Machines. This EDA step is crucial to understanding variables' relationships and identifying which variables influence our target. 

Once we understand the data, we prepare it for modeling. This includes partitioning the data with a 75-25 train-test split, performing necessary imputations, relevant centering and scaling, and more as outlined in our data exploration and preparation sections. When building our models we focus on using methods that produce real-world accuracy. For this reason, we attempt to select the best generalizable model with accuracy as our primary indicator during model evaluation.  

```{r libraries, include=FALSE, warning=FALSE, message=FALSE}
# Libraries
library(summarytools)
library(tidyverse)
library(DataExplorer)
library(reshape2)
library(mice)
library(caret)
library(MASS)
library(e1071)
library(tree)
library(corrplot)
library(kableExtra)
library(htmltools)
library(readxl)
library(psych)
library(xgboost)
library(ParBayesianOptimization)
library(factoextra)
library(kernlab)
set.seed(622)
```

# Data Exploration

The dataset with its column IDs, variable names, and variables descriptions are provided below for reference.

Columns | Variable | Description  
---|---|-----  
C |  Sex | Male-1, Female-2  
D | Race | White-1, African American-2, Hispanic-3, Asian-4, Native American-5, Other or missing data -6  
E - W | ADHD self-report scale |  Never-0, rarely-1, sometimes-2, often-3, very often-4  
X - AM | Mood disorder questions |  No-0, yes-1; question 3: no problem-0, minor-1, moderate-2, serious-3  
AN - AS | Individual substances misuse |   no use-0, use-1, abuse-2, dependence-3   
AT | Court Order |   No-0, Yes-1  
AU | Education |  1-12 grade, 13+ college  
AV | History of Violence |  No-0, Yes-1  
AW | Disorderly Conduct |  No-0, Yes-1  
AX | Suicide attempt |  No-0, Yes-1  
AY | Abuse Hx |  No-0, Physical (P)-1, Sexual (S)-2, Emotional (E)-3, P&S-4, P&E-5, S&E-6, P&S&E-7  
AZ | Non-substance-related Dx |  0 - none; 1 - one; 2 - More than one  
BA | Substance-related Dx |  0 - none; 1 - one Substance-related; 2 - two; 3 - three or more  
BB | Psychiatric Meds |  0 - none; 1 - one psychotropic med; 2 - more than one psychotropic med  

Notice how the data is grouped. There are labels for ADHD, Mood disorders, and Individual Substance misuse across a range of columns. These groups are reviewed throughout the exploration process and new features are generated to attempt to improve model performance. 

## Data Characteristics

```{r read-data}
# read data
adhd_data <- read_excel("ADHD_data.xlsx", sheet = "Data") %>% na_if("") %>% dplyr::select(-1)
#columns <- list(dimnames(adhd_data)[2])
#df <- adhd_data[,2:53]
adhd_data[,2:53] <- lapply(adhd_data[,2:53], factor)
adhd_data.dims <- dim(adhd_data)
```

The data contains `r adhd_data.dims[[1]]` observations of `r adhd_data.dims[[2]]` variables. We import the data from a remote repository and find that 51 of the variables should be of the factor data type given clear levels in their distributions. As is, these variables are interpreted as character strings. This will need to be converted for realistic results. The remaining variables can be numeric for our purposes. 

We review one grouped variable set, known as mood disorders (MD), to show what we're working with. These contain a series of associated questions (Q1-Q3) with Q1 containing parts 'a' through 'm.'

```{r md-data}
adhd_data[,c(23:37)]
```

Each part of Q1 'a' through 'm' corresponds with a specific question related to mood disorders for a single patient. In our feature engineering, it may be useful to tally these responses for a more holistic perspective of the patient's overall mood. We repeat this for the other groups to get an sense of the patient well-being which should provide insight into their risk of suicide.   

We examine the distribution of responses. This will inform us if there are any questions that are overly represented, missing values, or in need of adjustments or transformations prior to modeling. Assuming that we factorize responses that contain clear levels, we visualize the results as bars for each level of each variable.  


```{r, cat-bar, fig.length =20, fig.width=10}
# select categorical columns
cat_cols <- dimnames(adhd_data[,2:53])[[2]]
adhd_fact <-  adhd_data[cat_cols]
# long format
adhd_factm <- melt(adhd_fact, measure.vars = cat_cols, variable.name = 'metric', value.name = 'value')
# plot categorical columns
ggplot(adhd_factm, aes(x = value)) + 
  geom_bar(aes(fill = metric)) + 
  facet_wrap( ~ metric, nrow = 5L, scales = 'free') + coord_flip() + 
  theme(legend.position = "none")
```

The distributions of these values are spread somewhat evenly and should not require transformation at first glance. It is also clear that there are some missing values with various drug use reponses such as THC, Cocaine, Stimulants, Opioids, and more. 'Pschy Meds' med is missing the most values, with nearly all of its values listed as 'NA.' Since there are few missing values relative to the number of observations in the dataset, any basic imputation method should limit the effect on our predictions. 

We can visually tell that the data leans in certain directions which may influence how generalizable our results can be. For example, responses only include two racial groups, 1 and 2 with 2 being the most prominent. The rest have so few values their results may be insignificant. The data also leans towards sex 1, of binary-gender categories. There are more unbalanced occurrences such as this in questions for mood, education, abuse, disorderly conduct, substance use, psych med, and our target variable suicide. 

Adjustments may be necessary for some sub groups, for example in the responses of those with drug use, but the data are otherwise ready for use. We continue with a summary of the data. 


## Data summary

We identify any clusters of patients' responses that appear to be imbalanced by counting the frequencies of responses by level. This will provide us percentages of the previous bar chart where we could only compare bar sizes. It also shows if there is any duplication in responses. 

```{r adhd_data_summary}
dfSummary(adhd_data, style = 'grid', graph.col = FALSE)
```

Through this table we now know that some groups are more representative than others in the responses. Racial makeup is certainly limited in this study. Age might also be a limiting factor when ensuring predictive accuracy. However, duplication is not an issue. There appear to be no duplicates and the frequencies are close enough to normal that we can build realistic models with the data as it is. This confirms our characteristics. 

## Correlation

Next we will see the correlation among ADHD questions and MD questions. As we can deduce from below 2 correlation plots, ADHD questions are highly correlated and MD questions comparatively shows moderate correlation.

ADHD Correlation Plot:

```{r corr-adhds}
adhds <- sapply(adhd_data[,c(4:21)], as.numeric) %>% cor()
corrplot::corrplot(adhds, method="number")
```


Mood Correlation Plot:

```{r corr-mds}
mds <- sapply(adhd_data[,c(23:37)], as.numeric) %>% cor()
corrplot::corrplot(mds, method="number")
```

Within both groups, there are few, if any, particularly strong correlations that we could attribute to a patient's risk of suicide. Our most strongly correlated variables are found in the ADHD group which has a maximum correlation value of about 0.71 between Q7 and Q8. Everything else is lower but importantly, not zero. 

These correlations imply that the presence of answers to ADHD questions has a slightly higher correlation on the patient's risk of attempting suicide. Mood disorders have a lower set of correlations values with its maximum around 0.60. Although, plenty of these values are close enough to 0, which indicates no correlation, that we can say they have little no connection with a patients risk of attempting suicide. 

# Data Preparation

Given our EDA, we start preparing the data for modeling by handling missing values, analyzing which factors and factor levels would be most beneficial to the models, then preprocess and transform the data to fit model parameters when appropriate. We also split the data into training and test data sets for evaluating model performance. To complete preparation, we perform a principal component analysis (PCA) to compliment our factor analysis.  


## Factor Analysis

In Factor Analysis the dimensions of the data are reduced by picking a number of variables that explain most of the variation in the data. Those variables that  are not used are called latent variables because their values are inferred from other variables. This will help us further 
identify underlying factors that explain the correlation among the set of variables. Factor analysis is a great tool for treating multivariate questionnaire studies.

For ADHD questions, a test of the hypothesis determined that 3 factors are sufficient to explain the variation of the data. Our chi square statistic is 197.3 on 102 degrees of freedom. The p-value is 0.0000000476. Which, for our purposes, is higly significant. We used regression factor scores here as they predict the location of each individual on the factor. Results of this process are shown below. 

```{r adhd-ques-fa}
adhd_ques_fa <- factanal(sapply(adhd_data[,c(4:21)], as.numeric), 
                         factors = 3, 
                         rotation = "promax", 
                         scores = "regression")
adhd_ques_fa
```

To simplify the results from our regressive factanal call, this visual helps to describe which factors were used to build an our latent variables. The value listed on the arrow pointing to the ADHD question is its correlation with the original patient's response. 

Notice that none of the correlations are below 0.5 in factors 1 and 2. These are our most indicative questions for determining a patient's risk of attempting suicide. If we were looking to shorten the ADHD questionaire, these ADHD questions would be the best selection to retain predictive value in models. 

```{r adhd-fa-diag}
fa.diagram(adhd_ques_fa$loadings)
```


From the source data, we can see that for MD questions only the first MD question has multiple sub questions or parts. Compared to the second and third question, which have no additional parts. We will not need to perform fator analysis on questions with no additional parts because there are no additional factors to analyze.

We apply a similar factor analysis as of ADHD questions. Our hypothesis test showed that 3 factors were again sufficient. The chi square statistic is 88.82 on 63 degrees of freedom. The p-value is 0.0178. Significant to an alpha level below 0.05. Results of this test are shown below. 


```{r md-ques-fa}
md_ques_fa <- factanal(sapply(adhd_data[,c(23:37)], as.numeric), 
                         factors = 3, 
                         rotation = "promax", 
                         scores = "regression")
md_ques_fa
```



```{r mdfa-diag}
fa.diagram(md_ques_fa$loadings)
```

In the next step we will remove all ADHD question columns, ADHD Total, MD questions columns and MD TOTAL columns. Then we will add the new factors found above for ADHD and MD questions.


```{r add-fact}
# ADHD question scores dataframe
adhd_ques_fa <- as.data.frame(adhd_ques_fa$scores) 
names(adhd_ques_fa) <- c('ADHD_FACT1','ADHD_FACT2','ADHD_FACT3')

# MD questions scores dataframe
md_ques_fa <- as.data.frame(md_ques_fa$scores)
names(md_ques_fa) <- c('MD_FACT1','MD_FACT2','MD_FACT3')

# remove ADHD and MD columns
adhd_newdata <- adhd_data %>% dplyr::select(-c(starts_with('ADHD Q'), starts_with('MD Q')))

# Add new factor columns created
adhd_newdata <- cbind(adhd_newdata, adhd_ques_fa, md_ques_fa)
```

Here is glimpse of new set of data.

```{r head}
head(adhd_newdata)
```


```{r dim-red}
adhd_newdata.dims <- dim(adhd_newdata)
dim_reduced <- (53-25)/53
```

The dimensions of this new data set are 175 observations of 25 variables. We have successfully reduced the number of variables by `r round(dim_reduced, 2)`% while still explaining most of the variation in the data. Future questionnaires could be reduced to those variables listed in our new data set if they were in need of reducing the number of ADHD questions. 

## Handling missing values

When summarizing the data we discovered missing values for several of the variables. The variable psych med stood out the most since it was missing more than half of its data. Here, we visualize the percentage of missing values for all variables to compare to one another. Green bars indicate an insignificant level of missing values, blue could use imputation or another method to handle, and red is practically useless unless special precautions are taken during analysis.

```{r plot-missing}
# plot missing values
plot_missing(adhd_newdata)
```

We can see from this chart that `Psych meds`, in red, contributes to 67.43% of the missing data which is the maximum among all missing data in other columns. We will remove this column before imputation. We then impute values using MICE (Multivariate Imputation by Chained Equations) for columns that contain missing values. 

```{r rename}
# rename columns to apply mice
adhd_newdata <- adhd_newdata %>% 
  rename('ADHD_Total'='ADHD Total',
         'MD_Total'='MD TOTAL',
         'Sedative_hypnotics'='Sedative-hypnotics', 
         'Court_order' = 'Court order', 
         'Hx_of_Violence'='Hx of Violence', 
         'Disorderly_Conduct'='Disorderly Conduct', 
         'Non_subst_Dx'='Non-subst Dx',
         'Subst_Dx'='Subst Dx', 
         'Psych_meds'='Psych meds.') %>% 
  dplyr::select(-Psych_meds)
```



```{r impute}
# select columns with non missing values
temp <- adhd_newdata %>% dplyr::select(c(starts_with('ADHD_'), starts_with('MD_'), 'Race', 'Sex', 'Age'))

# impute predictors using mice
adhd_impute <- adhd_newdata %>% dplyr::select(-c(starts_with('ADHD_'), starts_with('MD_'), 'Race', 'Sex', 'Age'))
adhd_impute <- complete(mice(data=adhd_impute, print=FALSE))
summary(adhd_impute)
```


```{r merge}
# Merged the imputed dataframe with temp
adhd_newdata <- cbind(adhd_impute, temp)
head(adhd_newdata)
```



```{r cat-missing}
# Filter out 
#adhd_data <- adhd_data %>% filter(!is.na(Alcohol) &
#                                  !is.na(THC) &
#                                  !is.na(Cocaine) &
#                                  !is.na(Stimulants) &
#                                  !is.na(`Sedative-hypnotics`) &
#                                  !is.na(Opioids) &
#                                  !is.na(`Court order`) &
#                                  !is.na(Education) &
#                                  !is.na(`Hx of Violence`) &
#                                  !is.na(`Disorderly Conduct`) &
#                                  !is.na(Suicide) &
#                                  !is.na(Abuse) &
#                                  !is.na(`Non-subst Dx`) &
#                                  !is.na(`Subst Dx`) &
#                                  !is.na(`Psych meds.`))
```



```{r num-missing}
# impute numeric predictors using mice
#adhd_data <- complete(mice(data=adhd_data[,:53], method="pmm", print=FALSE))
```

From here, we have imputed or removed variables that contained missing values and identified those factors that could explain most of the data with fewer variables. We continue with some preprocessing steps. 


## Preprocess using transformation

There are minor changes necessary to fit and feed the models in our analysis. In our first transformation, we use dummyVars to create dummy variables for categorical features. Next we center and scale the data so that models can evaluate the data on the same dimensional planes. Otherwise, we lose any chance predicting with accuracy in real-world settings. 

```{r transform}
set.seed(622)
# create dummy variables for categorical features
adhd_dummy <- dummyVars(Suicide ~ ., data = adhd_newdata) 
adhd_dummy <- predict(adhd_dummy, newdata=adhd_newdata)

# center and scaling
adhd_transformed <- adhd_dummy %>% 
  preProcess(c("center", "scale")) %>% 
  predict(adhd_dummy) %>% 
  as.data.frame()

# add Suicide column
adhd_transformed$Suicide <- adhd_newdata$Suicide
```

We can review the transformations with the following table of values. These show all the variable's on the same scale with a center at zero. With this dataset it would be possible to supply data to the support vector machine and perform a proper clustering algorithm. 


```{r adhd_transformed_head}
head(adhd_transformed)
```


## Training and Test Partition

In this step for data preparation we will partition the training dataset in training and validation sets using `createDataPartition` method from the `caret` package. We will reserve 75% for training and rest 25% for validation purpose.

```{r partition}
set.seed(622)
partition <- createDataPartition(adhd_data$Suicide, p=0.75, list = FALSE)
training <- adhd_data[partition,]
testing <- adhd_data[-partition,]
# training/validation partition for independent variables
#X.train <- ld.clean[partition, ] %>% dplyr::select(-Loan_Status)
#X.test <- ld.clean[-partition, ] %>% dplyr::select(-Loan_Status)
# training/validation partition for dependent variable Loan_Status
#y.train <- ld.clean$Loan_Status[partition]
#y.test <- ld.clean$Loan_Status[-partition]
```


# Principal Component Analysis

Principal Compenent Analysis (PCA) is another way in which we can reduce the dimensionality of a data set while increasing the interpretability of the data and minimizing information loss. We are going to perform PCA for ADHD and MD response questions below while using scree plots to determine the numberof PCA's to keep. The Scree plot will display the eigenvalues in a downward curve, and order them from largest to smallest.

Groups: 

    * All ADHD Questions
    * All MD Questions
    * All Other

We begin with the ADHD Questions and produce appropriately titled visualizations to explain each variable's contribution to predicting the risk of attempting suicide within this analysis. 


```{r pca}
# create subset of ADHD Questions for PCA
adhd_ques_pca <- sapply(adhd_data[,c(4:21)], as.numeric)

# create subset of MD Questions for PCA
md_ques_pca <- sapply(adhd_data[,c(23:37)], as.numeric)
```


### ADHD

First we will use the prcomp function to perform a principal component analysis on the adhd response questions. We will also center and scale once again to ensure normality and proper dimensionality.


```{r prcomp}
pca_adhd <- prcomp(adhd_ques_pca, scale. = TRUE, center=TRUE)

```

We will use the `factoextra` library to display the results of our PCA. this library specializes in extracting and visualizing the out put of exploratory multivariate analysis. Through this and a correlation table we can see the relationship between each ADHD response score and the Principle Components. 

The list of PC's (sorted by descending impact on the variance of score) shows us the components that are the most impactful in grouping the respondents. By viewing the associated plots and correlations we can see the ADHd response questions `4,8,9,10,16,17,18` are the most impactful on plot of PC1 and PC2 which indicates they should be used in initial modeling of this dataset. 

We can also see the factors that are most impactful for other principal components below. Ensure to read the plot titles for a high-level view of what each plot describes. 


```{r corr}
cor(adhd_ques_pca, pca_adhd$x[,1:10]) %>% 
  kableExtra::kbl(booktabs = T, caption ="ADHD Correlations") %>% 
  kable_styling(latex_options = c("striped"), full_width = F) 
summary(pca_adhd)
fviz_eig(pca_adhd)
#top 10 contributors to the dimension of PC1 and PC2
fviz_contrib(pca_adhd, choice = "var", axes = c(1,2), top = 15)
fviz_pca_var(pca_adhd,
             col.var ="contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             ,axes=c(1,2)
             )
#top 10 contributors to the dimension of PC1 and PC3
fviz_contrib(pca_adhd, choice = "var", axes = c(1,3), top = 15)
fviz_pca_var(pca_adhd,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             ,axes=c(1,3)
             )
#top 10 contributors to the dimension of PC2 and PC3
fviz_contrib(pca_adhd, choice = "var", axes = c(2,3), top = 15)
fviz_pca_var(pca_adhd,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             ,axes=c(2,3)
             )
```

We will repeat the process above on MD response questions to get a better understanding of which of these questions are the most impactful. We can see that for PC1 and PC2 MD Questions 'Q1h', 'Q1j', 'Q1g', and 'Q2' have the greatest impact.

### MD
```{r md-prcomp}
pca_md <- prcomp(md_ques_pca, scale. = TRUE, center=TRUE)
cor(md_ques_pca, pca_md$x[,1:10]) %>% 
  kableExtra::kbl(booktabs = T, caption ="md Correlations") %>% 
  kable_styling(latex_options = c("striped"), full_width = F) 
summary(pca_md)
fviz_eig(pca_md)
#top 10 contributors to the dimension of PC1 and PC2
fviz_contrib(pca_md, choice = "var", axes = c(1,2), top = 15)
fviz_pca_var(pca_md,
             col.var ="contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             ,axes=c(1,2)
             )
#top 10 contributors to the dimension of PC1 and PC3
fviz_contrib(pca_md, choice = "var", axes = c(1,3), top = 15)
fviz_pca_var(pca_md,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             ,axes=c(1,3)
             )
#top 10 contributors to the dimension of PC2 and PC3
fviz_contrib(pca_md, choice = "var", axes = c(2,3), top = 15)
fviz_pca_var(pca_md,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             ,axes=c(2,3)
             )
```


# Gradient Boosting: Suicide

We assume we are modeling whether a patient attempted suicide codified in column AX. This known as a binary target variable. We use Gradient Boosting to predict whether a patient attempts suicides. Our specific approach uses an `XGBoost` of the `XGBoost` package library. 

Initially, we remove the rows null values in the target column and drop the Non-subset Dx column because it had a lot of nulls as well. XGBoost needs data to be in a matrix so we convert the dataframes to numeric matrices. 

We choose `XGBoost` because it is one of the more efficient gradient boosting methods that have some natural strengths at improving accuracy when used with questionnaire data such as ours. When constructing decision trees, variables that improve accuracy are selected for and 'boosted' to create trees that maximize accuracy. It does this without concern for how the leaves of the decision tree, exemplified by our sub-questions, get split into the branches of the decisions. Ideally, it will help us boost the accuracy of our model when trying to predict if a patient may attempt suicide. We continue this method with the first step, a cross validation (CV) split. 

```{r gb}
gb__train <-subset(training[complete.cases(training$Suicide), ], select= -`Non-subst Dx`)
gb__test <-subset(testing[complete.cases(testing$Suicide), ], select= -`Non-subst Dx`)
y_label_tr <- as.matrix(gb__train$Suicide)
y_label_test <- as.matrix(gb__test$Suicide)
gb__train <- sapply(subset(gb__train, select = -Suicide), as.numeric)
gb_test <- sapply(subset(gb__test, select = -Suicide), as.numeric)
```


## CV Split

We split the data into three folds for cross validation to improve the ability of the model to generalize and help with overfitting. We create a function to help with parameter tunning and make use of the `bayesOpt` package.  


```{r cv-spl}
Folds <- list(
    Fold1 = as.integer(seq(1,nrow(gb__train),by = 3))
  , Fold2 = as.integer(seq(2,nrow(gb__train),by = 3))
  , Fold3 = as.integer(seq(3,nrow(gb__train),by = 3))
)

scoringFunction <- function(max_depth, min_child_weight, subsample) {
  dtrain <- xgb.DMatrix(gb__train, label=y_label_tr)
  Pars <- list( 
      booster = "gbtree"
    , eta = 0.01
    , max_depth = max_depth
    , min_child_weight = min_child_weight
    , subsample = subsample
    , objective = "binary:logistic"
    , eval_metric = "auc"
  )
  xgbcv <- xgb.cv(
      params = Pars
    , data = dtrain
    , nround = 100
    , folds = Folds
    , prediction = TRUE
    , showsd = TRUE
    , early_stopping_rounds = 5
    , maximize = TRUE
            , verbose = 0)
  return(
    list( 
        Score = max(xgbcv$evaluation_log$test_auc_mean)
      , nrounds = xgbcv$best_iteration
    )
  )
}

```

Our `bayesOpt` package contained a set of functions that allowed us to optimize the model further, in a sense to improve the model's ability to reduce error and thus, improve predictive accuracy. We also create a scoring function that we use to perform the boosting on our target and evaluate its performance over rounds of testing. The results are shown for reference. 

```{r gb1}
set.seed(50)
bounds <- list( 
    max_depth = c(2L, 10L)
  , min_child_weight = c(1, 25)
  , subsample = c(0.25, .5)
)

optObj <- bayesOpt(
    FUN = scoringFunction
  , bounds = bounds
  , initPoints = 4
  , iters.n = 3
)
optObj$scoreSummary
print(getBestPars(optObj))
```

In several cases our model's scoring function appears to have a roughly 50% chance of attempting suicide. However, these cases all had fewer rounds of testing, shorter time elapsed, and used nonoptimum routes in its prediction. This is corrected with other tests in which further testing is completed through more rounds with heavier boosting. This is likely to produce a modest increase in our accuracy during model building. 


# Build Models

We develop three main models including a K-Clustering Method, Gradient Boosting method, and Support Vector Machines (SVM). Each focuses on a particular strategy. Our clustering attempts to identify groups of variables that are similar and use them to assess a patient's risk of attempting suicide. Gradient Boosting constructs decision trees with an emphasis on returning the most accurate decision path and our Support Vector Machine uses linear regression to classify the factors into groups using hyperplanes and by assessing the classes likelihood of being our target variable.  

## K-Clustering Method 1

We use K-nearest neighbor (KNN) to identify clusters of patients that share similar patterns that could help us predict our target variable. KNN works by identifying the "k" closest neighbors in the dataset. This works particularly well for classification of factorized data, as our dataset happens to be. Results of this model's construction are shown below. 

```{r cluster method knn}
set.seed(622)
mode <- function(x){
  levels <- unique(x)
  indicies <- tabulate(match(x, levels))
  levels[which.max(indicies)]
}
# Clean up training data
training_factors <- training %>% 
  dplyr::select(-Age, -`ADHD Total`, `MD TOTAL`) 
training_factors <- data.frame(lapply(training_factors, as.factor))
train_knn <- training_factors %>% 
  mutate(across(everything(), ~replace_na(., mode(.))))
mode(train_knn$Psych.meds.) 
train_knn$Psych.meds.[which(is.na(train_knn$Psych.meds.))] <- 0
sum(is.na(train_knn$Psych.meds.))

# Clean up testing data
testing_factors <- testing %>% 
  dplyr::select(-Age, -`ADHD Total`, `MD TOTAL`) 
testing_factors <- data.frame(lapply(testing_factors, as.factor))
test_knn <- testing_factors %>% 
  mutate(across(everything(), ~replace_na(., mode(.))))
mode(test_knn$Psych.meds.) 
test_knn$Psych.meds.[which(is.na(test_knn$Psych.meds.))] <- 0
sum(is.na(test_knn$Psych.meds.))

# Train KNN model
train.knn <- (train_knn[, names(train_knn) != "Suicide"])
prep <- preProcess(x = train.knn, method = c("center", "scale"))
cl <- trainControl(method="repeatedcv", repeats = 5) 
knn_model <- train(Suicide ~ ., data = train_knn, 
                method = "knn", 
                trControl = cl, 
                preProcess = c("center","scale"), 
                tuneLength = 20)
knn_model 
```


```{r knn-eval}
# Evaluate Model
plot(knn_model)
knn_predict <- predict(knn_model, newdata = test_knn)
mean(knn_predict == test_knn$Suicide) # accuracy
conf.mat.knn <- confusionMatrix(knn_predict, test_knn$Suicide)
accuracy <- round(conf.mat.knn$overall[[1]], 3)*100
conf.mat.knn
```

Our KNN model accuracy comes out to `r round(conf.mat.knn$overall[[1]], 3)*100`%. We note that the ideal number of k was experimentally determined by running through several simulated model and testing the accuracy of each. The most accurate model was our final choice. 

## K-Clustering Method 2

In this we use a special kind of K-means method where we attempt to identify clear grouping of factors and their respective levels across the dataset. To do this we isolate those factors, center and scale the values, and focus on the mean  euclidean distances between values. This method assumes there is an inherent similarity across values in the dataset including similarities with our target variable. The goal is to use these similarities (should there be any) to identify the groups closest to our target. If there are any groups, we would assume that they contain the patients who are at the highest risk of attempting suicide. Based on the data characteristics and summary from our EDA, we should not expect much from this model but it should prove informative.  


```{r clus-2}
set.seed(622)
# Clean up training data
training_factors <- training %>% 
  na.omit() %>% 
  dplyr::select(-Age, -`ADHD Total`, -`MD TOTAL`) 
training_factors <- data.frame(lapply(training_factors, as.factor))
train_heir <- training_factors %>% 
  mutate(across(everything(), ~replace_na(., mode(.))))
# mode(train_heir$Psych.meds.) 
# train_heir$Psych.meds.[which(is.na(train_heir$Psych.meds.))] 
# sum(is.na(train_heir))

# Clean up testing data
testing_factors <- testing %>% 
  na.omit() %>% 
  dplyr::select(-Age, -`ADHD Total`, -`MD TOTAL`) 
testing_factors <- data.frame(lapply(testing_factors, as.factor))
test_heir <- testing_factors %>% 
  mutate(across(everything(), ~replace_na(., mode(.))))
# mode(test_heir$Psych.meds.) 
# test_heir$Psych.meds.[which(is.na(test_heir$Psych.meds.))] 
# sum(is.na(test_heir$Psych.meds.))

# Train Heir model
train.heir <- (train_heir[, names(train_heir) != "Suicide"])
prep <- preProcess(x = train.heir, method = c("center", "scale"))
cl <- trainControl(method="repeatedcv", repeats = 5) 
heir_model <- train(Suicide ~ ., data = train_heir, 
                method = "knn", 
                trControl = cl, 
                preProcess = c("center","scale"), 
                tuneLength = 20)
heir_model 
```


```{r eval-model}
# Evaluate Model
plot(heir_model)
heir_predict <- predict(heir_model, newdata = test_heir)
conf.mat.heir <- confusionMatrix(heir_predict, test_heir$Suicide)
accuracy <- round(conf.mat.heir$overall[[1]], 3)*100
```

Our k value was experimentally determined to be significantly smaller for this model. As expected, the model's accuracy decreased since we were trying to group variable based similarities that we suspected would not be present in the data. However, this does confirm our expectations that there are fewer patterns between the target and all variables clustered together. When compared with smaller groups and more isolated variables, this tells us there may be a handful of important variables that predict our target better than all of them combined. 


## Heirarchical Clustering

In this we use a special kind of unsupervised learning method where we attempt to identify clear orders of factor and their respective levels across the dataset. To do this we isolate those factors, center and scale the values, remove our target from the data set, and review the results. Perhaps the easiest way to do this is with a dendrogram, as shown below. 

```{r hclus}
set.seed(622)

# Clean up training data
training_factors <- training %>% 
  na.omit() %>% 
  dplyr::select(-Age, -`ADHD Total`, -`MD TOTAL`) 
training_factors <- data.frame(lapply(training_factors, as.factor))
train_heir <- training_factors %>% 
  mutate(across(everything(), ~replace_na(., mode(.))))

# Clean up testing data
testing_factors <- testing %>% 
  na.omit() %>% 
  dplyr::select(-Age, -`ADHD Total`, -`MD TOTAL`) 
testing_factors <- data.frame(lapply(testing_factors, as.factor))
test_heir <- testing_factors %>% 
  mutate(across(everything(), ~replace_na(., mode(.))))

# Create Euclidean Dissimilarity Matrix 
train.heir <- (train_heir[, names(train_heir) != "Suicide"])
d <- dist(train.heir, method = "euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )
# Plot the obtained dendrogram
# Cut tree into groups
sub_grp <- cutree(hc1, k = 3)
train.heir %>%
  mutate(cluster = sub_grp) %>% 
  head()
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 3, border = 2:5)
```

Given that this dendrogram contains 53 variables, each labeled by their index value, we notice 3 large groups. These generally correspond to ADHD questions, Mood Disorder questions, and all other questions. However, there is a clear seperate branch of the dendrogram boxed in red. This mainly contains ADHD questions. This is interesting since it suggest the patient's responses to these questions have greater impact on the patient's risk of suicide. Alternatively, it could signify that the groups have little in common but at smaller sizes of k there may be significant predictors. We should learn more with the support vector machine. 


## Support Vector Machine

The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space (N being the number of features) that classifies the data points. Hyperplanes are decision boundaries to classify the data points. Data points that fall on either side of the hyperplane can qualify as different classes. Support vectors are data points that are closer to the hyperplane and effect the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier.

There are number of R packages available to implement SVM. The `train` function can be used for SVM using methods as `svmRadial`, `svmLinear` and `svmPoly` that fit different kernels. The functions we focus on are available in the `caret` package. 

```{r train-test}
# partitioning for train and test
partition <- createDataPartition(adhd_transformed$Suicide, p=0.75, list = FALSE)
training <- adhd_transformed[partition,]
testing <- adhd_transformed[-partition,]
```


```{r svm}
set.seed(622)

# fit with svmLinear
svm_lin_fit <- train(Suicide ~ ., 
                data = training, 
                method = "svmLinear",
                preProcess = c("center","scale"),
                tuneLength = 5,
                trControl = trainControl(method = "cv"))

pred_lin_suicide <- predict(svm_lin_fit, testing)
cm_lin <- confusionMatrix(testing$Suicide, pred_lin_suicide)

# fit with svmRadial
svm_rad_fit <- train(Suicide ~ ., 
                data = training, 
                method = "svmRadial",
                preProcess = c("center","scale"),
                tuneLength = 5,
                trControl = trainControl(method = "cv"))

pred_rad_suicide <- predict(svm_rad_fit, testing)
cm_rad <- confusionMatrix(testing$Suicide, pred_rad_suicide)

# fit with svmPoly
svm_poly_fit <- train(Suicide ~ ., 
                data = training, 
                method = "svmPoly",
                preProcess = c("center","scale"),
                tuneLength = 5,
                trControl = trainControl(method = "cv"))

pred_poly_suicide <- predict(svm_poly_fit, testing)
cm_poly <- confusionMatrix(testing$Suicide, pred_poly_suicide)

# Evaluate models
conf.mat.svnpoly <- confusionMatrix(pred_poly_suicide, testing$Suicide)
conf.mat.svnrad <- confusionMatrix(pred_rad_suicide, testing$Suicide)
conf.mat.svnlin <- confusionMatrix(pred_lin_suicide, testing$Suicide)
```



```{r svm-resam}
#Compare 3 models:
svm_resamps <- resamples(list(Linear = svm_lin_fit, Radial = svm_rad_fit, Poly = svm_poly_fit))
summary(svm_resamps)
```

We can see our Support Vector Machine Linear, Radial, and Poly fit had median accuracy rates of .631, .769 and .769 respectively indicating of radial or poly SVM should be chosen for future modeling.

**Most Important Features**

To better comprehend how the model came to it conclusion, it is beneficial to know which factors influence the target variable most. A plot of the top variables and their importance is displayed for review and comparison. 

```{r top-imp}
# Important features
imp_vars <- varImp(svm_rad_fit, scale=FALSE)
plot(imp_vars, top=10)
```


Abuse, alcoholism, and Mood Disorder Question 1 were the most important factors in the SVM models. 


## Gradient Boosted 

We use the information from the above function to fit our final model, make predictions, and evaluate results. This gradient boost relys on the data preparation and cross-validation split from our previous sections. 

```{r grad-boost}
dtrain <- xgb.DMatrix(gb__train, label=y_label_tr)
dtest <- xgb.DMatrix(gb_test, label=y_label_test)
xgb <- xgb.train(
      params = list( 
                  booster = "gbtree"
                , eta = 0.01
                , max_depth = 10
                , min_child_weight = 1
                , subsample = .5
                , objective = "binary:logistic"
                , eval_metric = "auc"
              )
    , data = dtrain
    , nround = 100
    , maximize = TRUE
            , verbose = 0)

xgbpred <- predict(xgb,dtest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
y_label_test <- as.numeric(y_label_test)
conf.mat.xgboost <- confusionMatrix(table(xgbpred, y_label_test))
conf.mat.xgboost
```

This produced an accuracy rate of 77.5%

# Model Performance

We can see that model Gradient Boosted model has the best accuracy at 77.5% when applied to the test dataset. The Models shorthand names are K-nearest neighbor (KNN), Hierarchical Clustering (HC), linear support vector machine (LIN), radial support vector machine (RAD), polynomial support vector machine (PLY), and the gradient XGBoosted model (XGB). We could improve these models through more through feature selection via PCA or other methods and by focusing on feature engineering by using what was identified by these methods.

```{r model-perf}
(round(data.frame(conf.mat.knn$overall, 
                  conf.mat.heir$overall,
                  conf.mat.svnlin$overall, 
                  conf.mat.svnrad$overall, 
                  conf.mat.svnpoly$overall, 
                  conf.mat.xgboost$overall), 3)) %>%
  rename(KNN = conf.mat.knn.overall, 
         HC = conf.mat.heir.overall,
         LIN = conf.mat.svnlin.overall,
         RAD = conf.mat.svnrad.overall,
         PLY = conf.mat.svnpoly.overall,
         XGB = conf.mat.xgboost.overall) %>% 
  t() %>%
  kbl(booktabs =  T, caption = "Model Performance") %>% 
  kable_styling(latex_options = c("striped, HOLD_position"), full_width = F) %>%
  footnote(c(
    "Variables from PCA & factor analysis considered in model performance evaluation"
  ))
```


# Conclusion

Through the use of feature engineering and different models we can see that there are numerous ways to approach a dataset such as this. Both models were better at predicting when a patient would not attempt to commit suicide, and not nearly as good at predicting when a patient would. Going forward it would be best to modify the model to focus on predicting when someone would attempt suicide. It is much more beneficial given the problem at hand to be over cautious and less accurate then to be more accurate but less cautious. Potentially using principle components could improve the model and focusing on feature engineering in regards to "positive" cases where the patient attempted suicide.



# References

https://cran.r-project.org/web/packages/ParBayesianOptimization/vignettes/tuningHyperparameters.html

https://towardsdatascience.com/what-is-the-difference-between-pca-and-factor-analysis-5362ef6fa6f9

https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1226&context=pare

http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/

https://uc-r.github.io/hc_clustering

https://rdrr.io/r/stats/prcomp.html

# Code Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```





























