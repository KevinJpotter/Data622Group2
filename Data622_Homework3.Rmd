---
title: "Data622 - Group2 - Homework3"
author: "Zachary Palmore, Kevin Potter, Amit Kapoor, Adam Gersowitz"
date: "10/2/2021"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
  html_document:
    fig_width: 15
    highlight: pygments
    number_sections: no
    theme: flatly
    toc: yes
    toc_float: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width = 10)
```


# Overview
In this project, the dataset used, is for Loan approval where the prediction will be done for Loan approval status using Linear Discriminant Analysis (LDA), K-nearest neighbor (KNN), Decision Trees and Random Forest models.

# R packages
We will use `r` for data modeling. All packages used for data exploration, visualization, preparation and modeling are listed in Code Appendix.

```{r libraries, include=FALSE, warning=FALSE, message=FALSE}
# Libraries

library(summarytools)
library(tidyverse)
library(DataExplorer)
library(reshape2)
library(mice)
library(caret)
library(MASS)
library(e1071)
library(caret)
library(corrplot)

set.seed(622)
```

# Data Exploration
Below is the description of the variables of interest in the data set.

|VARIABLE NAME|DESCRIPTION|
|--|----|
|Loan_ID|Unique Loan ID|
|Gender|Male/ Female|
|Married|Applicant married (Y/N)|
|Dependents|Number of dependents|
|Education|Applicant Education (Graduate/ Undergraduate)|
|Self_Employed|Self employed (Y/N)|
|ApplicantIncome|Applicant income|
|CoapplicantIncome|Coapplicant income|
|LoanAmount|Loan amount in thousands|
|Loan_Amount_Term|Term of loan in months|
|Credit_History|credit history meets guidelines|
|Property_Area|Urban/ Semi Urban/ Rural|
|Loan_Status|Loan approved (Y/N)|


```{r data}
# read data, change blank to NA and and remove loan_id
loan_data <- read.csv('https://raw.githubusercontent.com/amit-kapoor/Data622Group2/main/Loan_approval.csv') %>% 
  na_if("") %>%
  dplyr::select(-1)

# categorical columns as factors
loan_data <- loan_data %>% 
  mutate(Gender=as.factor(Gender),
         Married=as.factor(Married),
         Dependents=as.factor(Dependents),
         Education=as.factor(Education),
         Self_Employed=as.factor(Self_Employed),
         Property_Area=as.factor(Property_Area),
         Credit_History=as.factor(Credit_History),
         Loan_Status=as.factor(Loan_Status))

```


## Data summary

Below is summary of loan approval dataset.

```{r loan_data_summary}
dfSummary(loan_data, style = 'grid', graph.col = FALSE)
```

* There are 7 columns having missing values.
* The proportion of values for few columns shows significant differences i.e. Gender (more males), Married( more married), Credit_History (more having credit history).



Below graphs shows the distribution of all categorical variables.

```{r, cat-bar, fig.length =20, fig.width=10}

# select categorical columns
cat_cols = c()
j <- 1
for (i in 1:ncol(loan_data)) {
  if (class((loan_data[,i])) == 'factor') {
      cat_cols[j]=names(loan_data[i])
      j <- j+1
  }
}

loan_fact <-  loan_data[cat_cols]
# long format
loan_factm <- melt(loan_fact, measure.vars = cat_cols, variable.name = 'metric', value.name = 'value')

# plot categorical columns
ggplot(loan_factm, aes(x = value)) + 
  geom_bar() + 
  scale_fill_brewer(palette = "Set1") + 
  facet_wrap( ~ metric, nrow = 5L, scales = 'free') + coord_flip()
```


Below graph shows the distribution of numeric predictors.

```{r plot_num}
plot_histogram(loan_data, geom_histogram_args = list("fill" = "tomato4"))
```

Next we will cover impact of categorical variables on loan approval.

```{r}
loan_ch <- with(loan_data, table(Credit_History, Loan_Status)) %>% 
  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')

loan_ch
```

```{r}
ggplot(loan_ch, aes(x=Credit_History, y=Freq, fill=Credit_History)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Credit History', y = "Percentage", x = "Credit History")
```

```{r}
loan_gen <- with(loan_data, table(Gender, Loan_Status)) %>% 
  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')

loan_gen
```

```{r}
ggplot(loan_gen, aes(x=Gender, y=Freq, fill=Gender)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Gender', y = "Percentage", x = "Gender")
```

```{r}
loan_ed <- with(loan_data, table(Education, Loan_Status)) %>% 
  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')

loan_ed
```

```{r}
ggplot(loan_ed, aes(x=Education, y=Freq, fill=Education)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Education', y = "Percentage", x = "Education")
```


```{r}
loan_mar <- with(loan_data, table(Married, Loan_Status)) %>% 
  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')

loan_mar
```

```{r}
ggplot(loan_mar, aes(x=Married, y=Freq, fill=Married)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Married', y = "Percentage", x = "Married")
```


```{r}
loan_dep <- with(loan_data, table(Dependents, Loan_Status)) %>% 
  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')

loan_dep
```

```{r}
ggplot(loan_dep, aes(x=Dependents, y=Freq, fill=Dependents)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Dependents', y = "Percentage", x = "Dependents")
```



```{r}
G = cor(loan_data[6:(length(loan_data)-3)])
corrplot(G, method = 'number') # colorful number
```



# Data Preparation

## Handling missing values

```{r}
# plot missing values
plot_missing(loan_data)
```

We can see above credit_history contributes to 8% of missing data alongwith self_employed that accounts for more than 5% of missing data. All records having missing categorical predictors will be removed. Next we will impute numeric values using MICE (Multivariate Imputation by Chained Equations).

```{r cat-missing}
# Filter out the data which has missing categorical predictors
loan_data <- loan_data %>% filter(!is.na(Credit_History) &
                                  !is.na(Self_Employed) &  
                                  !is.na(Dependents) & 
                                  !is.na(Gender) & 
                                  !is.na(Married))
```



```{r num-missing}
# impute numeric predictors using mice
loan_data <- complete(mice(data=loan_data, method="pmm", print=FALSE))
```


```{r}
dim(loan_data)
```

Finally our clean dataset contains 511 rows and 12 columns.


## Preprocess using transformation

We have seen above that numeric features are right skewed so in this step we will use caret `preprocess` method using box cox, center and scale transformation.

```{r transform-train}
# library(e1071) - where this was used
set.seed(622)
loan_data <- loan_data %>% 
  dplyr::select(c("ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term")) %>%
  preProcess(method = c("BoxCox","center","scale")) %>% 
  predict(loan_data)
```


## Training and Test Partition

In this step for data preparation we will partition the training dataset in training and validation sets using `createDataPartition` method from `caret` package. We will reserve 75% for training and rest 25% for validation purpose.

```{r partition}
set.seed(622)
partition <- createDataPartition(loan_data$Loan_Status, p=0.75, list = FALSE)

training <- loan_data[partition,]
testing <- loan_data[-partition,]

# training/validation partition for independent variables
#X.train <- ld.clean[partition, ] %>% dplyr::select(-Loan_Status)
#X.test <- ld.clean[-partition, ] %>% dplyr::select(-Loan_Status)

# training/validation partition for dependent variable Loan_Status
#y.train <- ld.clean$Loan_Status[partition]
#y.test <- ld.clean$Loan_Status[-partition]
```


# Build Models

## Linear Discriminant Analysis (LDA)

```{r lda}
# LDA model
lda_model <- lda(Loan_Status~., data = loan_data)
lda_model
```

```{r}
# prediction from lda model
lda_predict <- lda_model %>% 
  predict(testing)
```


```{r lda-accuracy}
# accuracy
mean(lda_predict$class==testing$Loan_Status)
```

LDA model accuracy comes out as ~81%

## K-nearest neighbor (KNN)

```{r knn}
# KNN model
set.seed(622)
train.knn <- training[, names(training) != "Direction"]
prep <- preProcess(x = train.knn, method = c("center", "scale"))
prep
cl <- trainControl(method="repeatedcv", repeats = 5) 
knn_model <- train(Loan_Status ~ ., data = training, 
                method = "knn", 
                trControl = cl, 
                preProcess = c("center","scale"), 
                tuneLength = 20)
knn_model 
```



```{r}
# prediction from knn model
plot(knn_model)
knn_predict <- predict(knn_model,newdata = testing)
mean(knn_predict == testing$Loan_Status) # accuracy
confusionMatrix(knn_predict, testing$Loan_Status)
```



## Decision Trees

## Random Forests

# Model performance

```{r}

```


# Conclusion

# References

https://www.r-bloggers.com/2018/07/prop-table/

# Code Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```


















