---
title: "Data622 - Group2 - Homework3"
author: "Zachary Palmore, Kevin Potter, Amit Kapoor, Adam Gersowitz"
date: "10/2/2021"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
  html_document:
    fig_width: 15
    highlight: pygments
    number_sections: no
    theme: flatly
    toc: yes
    toc_float: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width = 10)
```


# Overview
In this project, the dataset used, is for Loan approval where the prediction will be done for Loan approval status using Linear Discriminant Analysis (LDA), K-nearest neighbor (KNN), Decision Trees and Random Forest models.

# R packages
We will use `r` for data modeling. All packages used for data exploration, visualization, preparation and modeling are listed in Code Appendix.

```{r libraries, include=FALSE, warning=FALSE, message=FALSE}
# Libraries

library(summarytools)
library(tidyverse)
library(DataExplorer)
library(reshape2)
library(mice)
library(caret)
library(MASS)
library(e1071)
library(caret)
library(tree)
library(randomForest)
library(corrplot)

set.seed(622)
```

# Data Exploration
Below is the description of the variables of interest in the data set.

|VARIABLE NAME|DESCRIPTION|
|--|----|
|Loan_ID|Unique Loan ID|
|Gender|Male/ Female|
|Married|Applicant married (Y/N)|
|Dependents|Number of dependents|
|Education|Applicant Education (Graduate/ Undergraduate)|
|Self_Employed|Self employed (Y/N)|
|ApplicantIncome|Applicant income|
|CoapplicantIncome|Coapplicant income|
|LoanAmount|Loan amount in thousands|
|Loan_Amount_Term|Term of loan in months|
|Credit_History|credit history meets guidelines|
|Property_Area|Urban/ Semi Urban/ Rural|
|Loan_Status|Loan approved (Y/N)|


```{r data}
# read data, change blank to NA and and remove loan_id
loan_data <- read.csv('https://raw.githubusercontent.com/amit-kapoor/Data622Group2/main/Loan_approval.csv') %>% 
  na_if("") %>%
  dplyr::select(-1)

# categorical columns as factors
loan_data <- loan_data %>% 
  mutate(Gender=as.factor(Gender),
         Married=as.factor(Married),
         Dependents=as.factor(Dependents),
         Education=as.factor(Education),
         Self_Employed=as.factor(Self_Employed),
         Property_Area=as.factor(Property_Area),
         Credit_History=as.factor(Credit_History),
         Loan_Status=as.factor(Loan_Status))

```


## Data summary

Below is summary of loan approval dataset.

```{r loan_data_summary}
dfSummary(loan_data, style = 'grid', graph.col = FALSE)
```


* There are 7 columns having missing values.
* The proportion of values for few columns shows significant differences i.e. Gender (more males), Married( more married), Credit_History (more having credit history).



Below graphs shows the distribution of all categorical variables.

```{r, cat-bar, fig.length =20, fig.width=10}

# select categorical columns
cat_cols = c()
j <- 1
for (i in 1:ncol(loan_data)) {
  if (class((loan_data[,i])) == 'factor') {
      cat_cols[j]=names(loan_data[i])
      j <- j+1
  }
}

loan_fact <-  loan_data[cat_cols]
# long format
loan_factm <- melt(loan_fact, measure.vars = cat_cols, variable.name = 'metric', value.name = 'value')

# plot categorical columns
ggplot(loan_factm, aes(x = value)) + 
  geom_bar() + 
  scale_fill_brewer(palette = "Set1") + 
  facet_wrap( ~ metric, nrow = 5L, scales = 'free') + coord_flip()
```


Below graph shows the distribution of numeric predictors.

```{r plot_num}
plot_histogram(loan_data, geom_histogram_args = list("fill" = "tomato4"))
```

Next we will cover impact of categorical variables on loan approval.

```{r}
loan_ch <- with(loan_data, table(Credit_History, Loan_Status)) %>% 
  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')

loan_ch
```

```{r}
ggplot(loan_ch, aes(x=Credit_History, y=Freq, fill=Credit_History)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Credit History', y = "Percentage", x = "Credit History")
```

```{r}
loan_gen <- with(loan_data, table(Gender, Loan_Status)) %>% 
  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')

loan_gen
```

```{r}
ggplot(loan_gen, aes(x=Gender, y=Freq, fill=Gender)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Gender', y = "Percentage", x = "Gender")
```

```{r}
loan_ed <- with(loan_data, table(Education, Loan_Status)) %>% 
  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')

loan_ed
```

```{r}
ggplot(loan_ed, aes(x=Education, y=Freq, fill=Education)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Education', y = "Percentage", x = "Education")
```


```{r}
loan_mar <- with(loan_data, table(Married, Loan_Status)) %>% 
  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')

loan_mar
```

```{r}
ggplot(loan_mar, aes(x=Married, y=Freq, fill=Married)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Married', y = "Percentage", x = "Married")
```


```{r}
loan_dep <- with(loan_data, table(Dependents, Loan_Status)) %>% 
  prop.table(margin = 1) %>% as.data.frame() %>% filter(Loan_Status == 'Y')

loan_dep
```

```{r}
ggplot(loan_dep, aes(x=Dependents, y=Freq, fill=Dependents)) + geom_bar(stat='identity') + labs(title = 'Approved Loans by Dependents', y = "Percentage", x = "Dependents")
```

```{r}

G = cor(loan_data[6:(length(loan_data)-3)])
corrplot(G, method = 'number') # colorful number
```

The numeric features do not seem to be strongly correlated with another so that is a factor that does not have to be dealt with.

```{r}
G = cor(loan_data[6:(length(loan_data)-3)])
corrplot(G, method = 'number') # colorful number
```



# Data Preparation

## Handling missing values

```{r}
# plot missing values
plot_missing(loan_data)
```

We can see above credit_history contributes to 8% of missing data along with self_employed that accounts for more than 5% of missing data. All records having missing categorical predictors will be removed. Next we will impute numeric values using MICE (Multivariate Imputation by Chained Equations).

```{r cat-missing}
# Filter out the data which has missing categorical predictors
loan_data <- loan_data %>% filter(!is.na(Credit_History) &
                                  !is.na(Self_Employed) &  
                                  !is.na(Dependents) & 
                                  !is.na(Gender) & 
                                  !is.na(Married))
```



```{r num-missing}
# impute numeric predictors using mice
loan_data <- complete(mice(data=loan_data, method="pmm", print=FALSE))
```


```{r}
dim(loan_data)
```

Finally our clean dataset contains 511 rows and 12 columns.


## Preprocess using transformation

We have seen above that numeric features are right skewed so in this step we will use caret `preprocess` method using box cox, center and scale transformation.

```{r transform-train}
# library(e1071) - where this was used
set.seed(622)
loan_data <- loan_data %>% 
  dplyr::select(c("ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term")) %>%
  preProcess(method = c("BoxCox","center","scale")) %>% 
  predict(loan_data)
```


## Training and Test Partition

In this step for data preparation we will partition the training dataset in training and validation sets using `createDataPartition` method from `caret` package. We will reserve 75% for training and rest 25% for validation purpose.

```{r partition}
set.seed(622)
partition <- createDataPartition(loan_data$Loan_Status, p=0.75, list = FALSE)

training <- loan_data[partition,]
testing <- loan_data[-partition,]

# training/validation partition for independent variables
#X.train <- ld.clean[partition, ] %>% dplyr::select(-Loan_Status)
#X.test <- ld.clean[-partition, ] %>% dplyr::select(-Loan_Status)

# training/validation partition for dependent variable Loan_Status
#y.train <- ld.clean$Loan_Status[partition]
#y.test <- ld.clean$Loan_Status[-partition]
```


# Build Models

## Linear Discriminant Analysis (LDA)

```{r lda}
# LDA model
lda_model <- lda(Loan_Status~., data = loan_data)
lda_model
```

```{r}
# prediction from lda model
lda_predict <- lda_model %>% 
  predict(testing)
```


```{r lda-accuracy}
# accuracy
mean(lda_predict$class==testing$Loan_Status)
confusionMatrix(lda_predict$class, testing$Loan_Status)
```

LDA model accuracy comes out as ~81%

## K-nearest neighbor (KNN)

```{r knn}
# KNN model
set.seed(622)
train.knn <- training[, names(training) != "Direction"]
prep <- preProcess(x = train.knn, method = c("center", "scale"))
prep
cl <- trainControl(method="repeatedcv", repeats = 5) 
knn_model <- train(Loan_Status ~ ., data = training, 
                method = "knn", 
                trControl = cl, 
                preProcess = c("center","scale"), 
                tuneLength = 20)
knn_model 
```



```{r}
# prediction from knn model
plot(knn_model)
knn_predict <- predict(knn_model,newdata = testing)
mean(knn_predict == testing$Loan_Status) # accuracy
confusionMatrix(knn_predict, testing$Loan_Status)
```

KNN model accuracy comes out as ~80%


## Decision Trees

```{r Decision}
# Decision Trees model
set.seed(622)
tree.loans = tree(Loan_Status~., data=training)
summary(tree.loans)
plot(tree.loans)
text(tree.loans, pretty = 0)
```

```{r}
# prediction from decision tree model
tree.predict<-predict(tree.loans, testing, type = 'class')
mean(tree.predict == testing$Loan_Status) # accuracy
confusionMatrix(tree.predict, testing$Loan_Status)
```

Decision Tree model accuracy comes out as ~80%

## Random Forests

```{r rf}
set.seed(622)
# Random Forest model
rf.loans <- randomForest(Loan_Status~., data = training)
rf.loans

```

```{r}
# prediction from random forest model
rf.predict <- predict(rf.loans, testing,type='class')
mean(rf.predict == testing$Loan_Status) # accuracy
confusionMatrix(rf.predict, testing$Loan_Status)
```

Random Forest model accuracy comes out as ~80%

# Model Performance

All 4 of the models we built above have an accuracy rate of around 80% with the LDA model getting the light edge in accuracy (81.1% to 79.5% for the other 3). 

Next we will look at some more detailed accuracy metrics produced from the predict function. 

In this particular case, since the accuracy results are so similar it is wise to examine which models are most often leading to type 1 or type 2 errors. Assuming a null hypothesis is not giving a loan a type 1 error is giving someone a loan when they should not have gotten one and a type 2 error is not giving someone a loan when they should have gotten one. This correlates to the sensitivity and specificity respectively. The balanced accuracy metric takes the mean of sensitivity and specificity in order to diagnose if a model appears to be accurate but is really only predicting the positive or negative case correctly. In this case the random forest model has a slight edge over the LDA in balanced accuracy (0.7212 to 0.7201).

Although the LDA has the smallest p-value it is still <0.0025 for all models so they can all be used so this does not help us decide which model to choose. The same can be said for the Mcnemar's p-value.

The LDA also has the best Kappa score of 0.5046. The Kappa score can be a much more accurate indicator of accuracy then the standard accuracy rate. The kappa score takes into account the expected accuracy of a model given the instances of each class. This helps a lot with unbalanced class numbers in the dataset. It is encouraging to see the LDA Kappa is also the highest which corresponds with it having the highest accuracy. Although the accuracy score is high a Kappa score around 0.5046 only indicates a moderately good model.

Given the LDa has the highest accuracy, kappa, and nearly the highest balanced accuracy, the LDA is the model that we would use going forward.

# Conclusion

After reviewing the results of 4 different models (LDA, KNN, Decision Tree, and Random Forest), we found that LDA is the most accurate by numerous metrics and should be considered the best model for this data. However, the metrics for these 4 models was very close and it did not leave us with no choice but to pick the LDA. If presenting this to stakeholders we would recommend the LDA model with the caveat they further testing and verification may be needed to ensure the LDA is the most accurate model of other iterations of the split of training and testing data.

# References


https://www.r-bloggers.com/2018/07/prop-table/

https://www.datacamp.com/community/tutorials/decision-trees-R

https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english

# Code Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```

















